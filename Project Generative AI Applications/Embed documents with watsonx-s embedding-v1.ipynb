{"cells":[{"cell_type":"markdown","id":"0b997819-1859-4e1f-b575-d145d8b9d6d5","metadata":{},"outputs":[],"source":["<p style=\"text-align:center\">\n","    <a href=\"https://skills.network\" target=\"_blank\">\n","    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"200\" alt=\"Skills Network Logo\"  />\n","    </a>\n","</p>\n"]},{"cell_type":"markdown","id":"d33c22f1-1782-4b64-ad08-0de2005c046e","metadata":{},"outputs":[],"source":["# **Embed documents using watsonx's embedding model**\n"]},{"cell_type":"markdown","id":"b72fd328-3118-4f4b-94e5-9e25e48d7df1","metadata":{},"outputs":[],"source":["Estimated time needed: **30** minutes\n"]},{"cell_type":"markdown","id":"9e5e9c2b-e5fb-4733-897e-59ae2c42edbf","metadata":{},"outputs":[],"source":["## Overview\n"]},{"cell_type":"markdown","id":"ead2398e-7752-4a8f-94b9-3b16b0fe8fcc","metadata":{},"outputs":[],"source":["Imagine you work in a company that handles a vast amount of text data, including documents, emails, and reports. Your task is to build an intelligent search system that can quickly and accurately retrieve relevant documents based on user queries. Traditional keyword-based search methods often fail to understand the context and semantics of the queries, leading to poor search results.\n","\n","To address this challenge, you can use embedding models to convert documents into numerical vectors. These vectors capture the semantic meaning of the text, enabling more accurate and context-aware search capabilities. Document embedding is a powerful technique to convert textual data into numerical vectors, which can then be used for various downstream tasks such as search, classification, clustering, and more.\n"]},{"cell_type":"markdown","id":"648fea0c-6ce7-446c-b526-bb41ff4166fe","metadata":{},"outputs":[],"source":["<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/O_XVADBArH0ck4Wf6uuoBA/embeddings.png\" width=\"60%\" alt=\"embeddings\">\n"]},{"cell_type":"markdown","id":"b53e53c9-ef39-4181-972e-3a75ae2f9c17","metadata":{},"outputs":[],"source":["In this lab, you will learn how to use embedding models from watsonx.ai and Hugging Face to embed documents. By the end of this lab, you will be able to effectively use these embedding models to transform and utilize textual data in your projects.\n"]},{"cell_type":"markdown","id":"1f0adcc3-cec4-4df3-a478-a55a914a8010","metadata":{},"outputs":[],"source":["## __Table of Contents__\n","\n","<ol>\n","    <li><a href=\"#Objectives\">Objectives</a></li>\n","    <li>\n","        <a href=\"#Setup\">Setup</a>\n","        <ol>\n","            <li><a href=\"#Installing-required-ribraries\">Installing required libraries</a></li>\n","            <li><a href=\"#Load-data\">Load data</a></li>\n","            <li><a href=\"#Split data\">Split data</a></li>\n","        </ol>\n","    </li>\n","    <li>\n","        <a href=\"#Watsonx-embedding-model\">Watsonx embedding model</a>\n","        <ol>\n","            <li><a href=\"#Model-description\">Model description</a></li>\n","            <li><a href=\"#Build-model\">Build model</a></li>\n","            <li><a href=\"#Query-embeddings\">Query embeddings</a></li>\n","            <li><a href=\"#Document-embeddings\">Document embeddings</a></li>\n","        </ol>\n","    </li>\n","    <li>\n","        <a href=\"#HuggingFace embedding model\">HuggingFace embedding models</a>\n","        <ol>\n","            <li><a href=\"#Model-description\">Model description</a></li>\n","            <li><a href=\"#Build-model\">Build model</a></li>\n","            <li><a href=\"#Query-embeddings\">Query embeddings</a></li>\n","            <li><a href=\"#Document-embeddings\">Document embeddings</a></li>\n","        </ol>\n","    </li>\n","</ol>\n","\n","<a href=\"#Exercises\">Exercises</a>\n","<ol>\n","    <li><a href=\"#Exercise-1---Using-another-watsonx-embedding-model\">Exercise 1. Using another watsonx embedding model</a></li>\n","</ol>\n"]},{"cell_type":"markdown","id":"5c0e1569-53af-4eae-ad5a-49b46bb11dee","metadata":{},"outputs":[],"source":["## Objectives\n","\n","After completing this lab, you will be able to:\n","\n"," - Prepare and preprocess documents for embedding\n"," - Use watsonx.ai and Hugging Face embedding models to generate embeddings for your documents\n"]},{"cell_type":"markdown","id":"ae49a522-8a42-4aca-a893-2d08396040fa","metadata":{},"outputs":[],"source":["----\n"]},{"cell_type":"markdown","id":"f2688581-d7fa-488d-93bf-d83d47923f19","metadata":{},"outputs":[],"source":["## Setup\n"]},{"cell_type":"markdown","id":"259dadde-1563-4b8e-9b4a-734bec851e1e","metadata":{},"outputs":[],"source":["For this lab, you will use the following libraries:\n","\n","* [`ibm-watson-ai`](https://ibm.github.io/watsonx-ai-python-sdk/fm_embeddings.html#EmbeddingModels) for using embedding models from IBM's watsonx.ai.\n","* [`langchain`, `langchain-ibm`, `langchain-community`](https://www.langchain.com/) for using relevant features from LangChain.\n","* [`sentence-transformers`](https://huggingface.co/sentence-transformers) for using embedding models from HuggingFace.\n"]},{"cell_type":"markdown","id":"6f56e7b2-924b-49c9-b500-e0d02c24bb21","metadata":{},"outputs":[],"source":["### Installing required libraries\n","\n","The following required libraries are __not__ preinstalled in the Skills Network Labs environment. __You need to run the following cell__ to install them:\n","\n","**Note:** The version is being pinned here to specify the version. It's recommended that you do this as well. Even if the library is updated in the future, the installed library could still support this lab work.\n","\n","This might take around 1-2 minutes. \n","\n","As `%%capture` is used to capture the installation, you won't see the output process. But after the installation completes, you will see a number beside the cell.\n"]},{"cell_type":"code","id":"84908bc3-3b3f-4469-9261-a60e51b2a835","metadata":{},"outputs":[],"source":["%%capture\n!pip install \"ibm-watsonx-ai==1.1.2\"\n!pip install \"langchain==0.2.11\"\n!pip install \"langchain-ibm==0.1.11\"\n!pip install \"langchain-community==0.2.10\"\n!pip install \"sentence-transformers==3.0.1\""]},{"cell_type":"markdown","id":"2aaba4ee-2366-432f-a87a-67ce60d4ed00","metadata":{},"outputs":[],"source":["After you install the libraries, restart your kernel. You can do that by clicking the **Restart the kernel** icon.\n","\n","<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/build-a-hotdog-not-hotdog-classifier-guided-project/images/Restarting_the_Kernel.png\" width=\"50%\" alt=\"Restart kernel\">\n"]},{"cell_type":"markdown","id":"52411e51-4de6-45af-9ee2-605411b6ef3b","metadata":{},"outputs":[],"source":["## Load data\n"]},{"cell_type":"markdown","id":"f341ad35-793e-46ac-a52c-97a4fd08a855","metadata":{},"outputs":[],"source":["A text file has been prepared as the source document for the downstream embedding task.\n","\n","Now, let's download and load it using LangChain's `TextLoader`.\n"]},{"cell_type":"code","id":"c808cb62-cc31-44be-9466-c5b6efd4a535","metadata":{},"outputs":[],"source":["!wget \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/i5V3ACEyz6hnYpVq6MTSvg/state-of-the-union.txt\""]},{"cell_type":"code","id":"6cf726f6-1af0-4013-ae9a-8614e6e007f5","metadata":{},"outputs":[],"source":["from langchain_community.document_loaders import TextLoader"]},{"cell_type":"code","id":"9cf5ed3a-56e9-4b1d-bbaf-9c6d8146f78a","metadata":{},"outputs":[],"source":["loader = TextLoader(\"state-of-the-union.txt\")\ndata = loader.load()"]},{"cell_type":"markdown","id":"6b34d0df-a750-4e35-8b1a-76e17ad9c3df","metadata":{},"outputs":[],"source":["Let's take a look at the document.\n"]},{"cell_type":"code","id":"eea885ba-9cb5-49b5-8e2a-6eb27a36549f","metadata":{},"outputs":[],"source":["data"]},{"cell_type":"markdown","id":"39c3351e-7029-423a-b2de-dbd2258cefe4","metadata":{},"outputs":[],"source":["## Split data\n"]},{"cell_type":"markdown","id":"aa86ebed-ab37-45e5-922f-6a2fa8f35323","metadata":{},"outputs":[],"source":["Since the embedding model has a maximum input token limit, you cannot input the entire document at once. Instead, you need to split it into chunks.\n","\n","The following code shows how to use LangChain's `RecursiveCharacterTextSplitter` to split the document into chunks.\n","- Use the default separator list, which is `[\"\\n\\n\", \"\\n\", \" \", \"\"]`.\n","- Chunk size is set to `100`. This should be set to less than the model's maximum input token.\n","- Chunk overlap is set to `20`.\n","- The length function is `len`.\n"]},{"cell_type":"code","id":"f62382e5-baba-4252-a35c-d58687577d09","metadata":{},"outputs":[],"source":["from langchain.text_splitter import RecursiveCharacterTextSplitter"]},{"cell_type":"code","id":"433fda95-b6a1-4705-ade6-dd60e90f15e9","metadata":{},"outputs":[],"source":["text_splitter = RecursiveCharacterTextSplitter(\n    chunk_size=100,\n    chunk_overlap=20,\n    length_function=len,\n)"]},{"cell_type":"code","id":"fc654f31-79a0-4332-b9e5-789e81044eb3","metadata":{},"outputs":[],"source":["chunks = text_splitter.split_text(data[0].page_content)"]},{"cell_type":"markdown","id":"2ad31973-c492-4173-a78c-20288a51ee05","metadata":{},"outputs":[],"source":["Let's see how many chunks you get.\n"]},{"cell_type":"code","id":"0136b2dd-6478-4424-ab8a-a8c65dcafe18","metadata":{},"outputs":[],"source":["len(chunks)"]},{"cell_type":"markdown","id":"613fff8e-2550-400d-9335-b69a3e12c202","metadata":{},"outputs":[],"source":["Let's also see what these chunks looks like.\n"]},{"cell_type":"code","id":"fe88f063-7907-4d37-a31f-72d35ebf655f","metadata":{},"outputs":[],"source":["chunks"]},{"cell_type":"markdown","id":"d52dfa58-ab7a-45fb-8035-c52f091a6c59","metadata":{},"outputs":[],"source":["## Watsonx embedding model\n"]},{"cell_type":"markdown","id":"7297c631-fa70-47f2-8732-6b00fe9ba2ca","metadata":{},"outputs":[],"source":["### Model description\n"]},{"cell_type":"markdown","id":"32afcd3c-3145-453f-8e85-7ee65cd1b8d5","metadata":{},"outputs":[],"source":["In this section, you will use IBM `slate-125m-english-rtrvr` model as an example embedding model.\n","\n","The slate.125m.english.rtrvr model is a [standard sentence](https://www.sbert.net/) transformers model based on bi-encoders. The model produces an embedding for a given input, e.g., query, passage, document, etc. At a high level, the model is trained to maximize the cosine similarity between two input pieces of text, e.g., text A (query text) and text B (passage text), which results in the sentence embeddings q and p.These sentence embeddings can be compared using cosine similarity, which measures the distance between sentences by calculating the distance between their embeddings.\n"]},{"cell_type":"markdown","id":"bfefcdf4-6db8-4fdd-8fb3-348f02f88d7e","metadata":{},"outputs":[],"source":["<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/NDCHhZfcC96jggb2hMdJhg/fm-slate-125m-english-rtrvr-cosine.jpg\" width=\"50%\">\n"]},{"cell_type":"markdown","id":"c5ee147b-3eb8-48f8-87d1-e3c0b6c8cbd6","metadata":{},"outputs":[],"source":["The embedding model, `slate.125m.english` formerly known as WatBERT, has the same architecture as a RoBERTa base transformer model and has ~125 million parameters and an embedding dimension of `768`.\n"]},{"cell_type":"markdown","id":"c523789f-f5be-46f5-86ae-5fab977b1f51","metadata":{},"outputs":[],"source":["|Model name|API model_id|Maximum input tokens|Number of dimensions|More information|\n","|-|-|-|-|-|\n","|slate-125m-english-rtrvr|ibm/slate-125m-english-rtrvr|512|768|[model card](https://dataplatform.cloud.ibm.com/docs/content/wsj/analyze-data/fm-slate-125m-english-rtrvr-model-card.html?utm_source=skills_network&utm_content=in_lab_content_link&utm_id=Lab-Embed+documents+with+watsonx%E2%80%99s+embedding_v1_1721662184&context=wx)|\n"]},{"cell_type":"markdown","id":"73fab66b-503b-401d-9147-2eaa7ca517e7","metadata":{},"outputs":[],"source":["### Build model\n"]},{"cell_type":"markdown","id":"ef5d367f-6ff4-48fb-a499-bf9c413897d7","metadata":{},"outputs":[],"source":["The following code shows how to build the `slate-125m-english-rtrvr` model from IBM watsonx.ai API.\n"]},{"cell_type":"markdown","id":"991af714-6b5a-4556-ad90-5e2ea40027b1","metadata":{},"outputs":[],"source":["First, import the necessary dependencies. \n","- `WatsonxEmbeddings` is a class/dependence that can be used to form an embedding model object.\n","- `EmbedTextParamsMetaNames` is a dependence that controls the embedding parameters.\n"]},{"cell_type":"code","id":"d1570334-a88c-4bc0-9ea0-d335c229f5e4","metadata":{},"outputs":[],"source":["from ibm_watsonx_ai.metanames import EmbedTextParamsMetaNames\nfrom langchain_ibm import WatsonxEmbeddings"]},{"cell_type":"code","id":"0ddd0881-da1a-46a6-96e5-6e10f3cee295","metadata":{},"outputs":[],"source":["embed_params = {\n    EmbedTextParamsMetaNames.TRUNCATE_INPUT_TOKENS: 3,\n    EmbedTextParamsMetaNames.RETURN_OPTIONS: {\"input_text\": True},\n}\n\nwatsonx_embedding = WatsonxEmbeddings(\n    model_id=\"ibm/slate-125m-english-rtrvr\",\n    url=\"https://us-south.ml.cloud.ibm.com\",\n    project_id=\"skills-network\",\n    params=embed_params,\n)"]},{"cell_type":"markdown","id":"0099c48e-eaac-4fc8-bf0f-9552d2c4a3e2","metadata":{},"outputs":[],"source":["### Query embeddings\n"]},{"cell_type":"markdown","id":"8be4b301-0c58-44fd-9031-d29f3ad975f5","metadata":{},"outputs":[],"source":["Now, create an embedding based on a single sentence, which can be treated as a query.\n"]},{"cell_type":"markdown","id":"da61c34c-2526-48c2-91cc-7453f26e7a60","metadata":{},"outputs":[],"source":["Use the `embed_query` method.\n"]},{"cell_type":"code","id":"134fcaee-6ece-488b-aba5-71bc84725aaf","metadata":{},"outputs":[],"source":["query = \"How are you?\"\n\nquery_result = watsonx_embedding.embed_query(query)"]},{"cell_type":"markdown","id":"5c048e70-8841-449a-9064-f3cd0efa0671","metadata":{},"outputs":[],"source":["Let's see the length/dimension of this embedding.\n"]},{"cell_type":"code","id":"d8f434ca-cbab-49ec-85e3-31c89fa179bb","metadata":{},"outputs":[],"source":["len(query_result)"]},{"cell_type":"markdown","id":"e3a5e587-51dc-4278-a9d6-14d9d6d4565f","metadata":{},"outputs":[],"source":["It has a dimension of `768`, which aligns with the model description. \n"]},{"cell_type":"markdown","id":"336cc47c-ece7-40b0-8df0-17f9d040288f","metadata":{},"outputs":[],"source":["Next, take a look at the first five results from the embeddings.\n"]},{"cell_type":"code","id":"3b105ed3-ee46-40ec-8832-4352f6e62516","metadata":{},"outputs":[],"source":["query_result[:5]"]},{"cell_type":"markdown","id":"a2d29505-52df-4f5e-a1ec-a0abd09e2d7f","metadata":{},"outputs":[],"source":["### Document embeddings\n"]},{"cell_type":"markdown","id":"0da6853f-7b41-4d77-b949-c61ea30eb06c","metadata":{},"outputs":[],"source":["After creating the query embeddings, you will be guided on how to create embeddings from documents, which are a list a text chunks.\n"]},{"cell_type":"markdown","id":"f9fd8a8b-adb5-422c-96ea-0e80372ea4b5","metadata":{},"outputs":[],"source":["Use `embed_documents`. The parameter `chunks` should be a list of text. Here, chunks is a list of documents you get from before after splitting the whole document.\n"]},{"cell_type":"code","id":"9e8ab784-8832-4fcc-aa5f-e64a9da28b79","metadata":{},"outputs":[],"source":["doc_result = watsonx_embedding.embed_documents(chunks)"]},{"cell_type":"markdown","id":"2607cf48-2c38-4d82-b01a-a97dca40c9b8","metadata":{},"outputs":[],"source":["As each piece of text is embedded into a vector, so the length of the `doc_result` should be the same as the length of chunks.\n"]},{"cell_type":"code","id":"c159b609-645e-4e2c-9632-15da13b24752","metadata":{},"outputs":[],"source":["len(doc_result)"]},{"cell_type":"markdown","id":"3ff01dcc-6de4-4b24-ba2a-f15abc148304","metadata":{},"outputs":[],"source":["Now, take a look at the first five results from the embeddings of the first piece of text.\n"]},{"cell_type":"code","id":"5862e76a-dfea-4f86-bdeb-eef33c6f35c2","metadata":{},"outputs":[],"source":["doc_result[0][:5]"]},{"cell_type":"markdown","id":"80200758-bb19-4eb9-93a5-cdb9e647e7fa","metadata":{},"outputs":[],"source":["Check the embedding dimension to see if it is also 768.\n"]},{"cell_type":"code","id":"8cb29236-1f08-4274-99a1-8efbb9b0fddf","metadata":{},"outputs":[],"source":["len(doc_result[0])"]},{"cell_type":"markdown","id":"b6e427db-97f1-4ad7-a665-68558ad944f8","metadata":{},"outputs":[],"source":["## Hugging Face embedding model\n"]},{"cell_type":"markdown","id":"73ae5c3b-9abe-4cd9-b235-02551d14dd58","metadata":{},"outputs":[],"source":["### Model description\n"]},{"cell_type":"markdown","id":"924cfc74-c502-4380-85cb-c85da77b5389","metadata":{},"outputs":[],"source":["In this section, you will use the `all-mpnet-base-v2` from HuggingFace as an example embedding model.\n","\n","It is a sentence-transformers model. It maps sentences and paragraphs to a 768-dimensional dense vector space and can be used for tasks like clustering or semantic search. It used the pre-trained `Microsoft/money-base` model and fine-tuned it on a 1B sentence pairs dataset. For more information, please refer to [here](https://huggingface.co/sentence-transformers/all-mpnet-base-v2).\n"]},{"cell_type":"markdown","id":"563dffd1-2bc4-4cd2-88dd-c7d73060c8a3","metadata":{},"outputs":[],"source":["### Build model\n"]},{"cell_type":"markdown","id":"4b9ee086-bc0d-44ca-95a8-c0c542458495","metadata":{},"outputs":[],"source":["To build the model, you need to import the `HuggingFaceEmbeddings` dependence first.\n"]},{"cell_type":"code","id":"f2c98b41-f010-49c8-8019-b13f05e89469","metadata":{},"outputs":[],"source":["from langchain_community.embeddings import HuggingFaceEmbeddings"]},{"cell_type":"markdown","id":"03300ed3-184f-4f40-ac52-0112e549391c","metadata":{},"outputs":[],"source":["Then, you specify the model name.\n"]},{"cell_type":"code","id":"03cc4ef6-1d3c-4c94-9117-920c6f889257","metadata":{},"outputs":[],"source":["model_name = \"sentence-transformers/all-mpnet-base-v2\""]},{"cell_type":"markdown","id":"2ca58762-97c0-4021-8e62-c535eefaf957","metadata":{},"outputs":[],"source":["Here we create a embedding model object.\n"]},{"cell_type":"code","id":"5ad7f1f4-ad29-4daf-bbd9-027923b553cf","metadata":{},"outputs":[],"source":["huggingface_embedding = HuggingFaceEmbeddings(model_name=model_name)"]},{"cell_type":"markdown","id":"9ffe3617-841c-4194-b63f-375eaa73a6a7","metadata":{},"outputs":[],"source":["### Query embeddings\n"]},{"cell_type":"markdown","id":"4386616c-2783-413e-8d58-c1b933bfd65c","metadata":{},"outputs":[],"source":["Let's create the embeddings from the same sentence, but using the Hugging Face embedding model. \n"]},{"cell_type":"code","id":"549fc5cc-ab0b-437d-8565-3507705ecf76","metadata":{},"outputs":[],"source":["query = \"How are you?\""]},{"cell_type":"code","id":"ec45970f-ec03-44a9-84c1-5d9189b860c8","metadata":{},"outputs":[],"source":["query_result = huggingface_embedding.embed_query(query)"]},{"cell_type":"code","id":"31a1d56d-5e30-4d17-b77b-17daa30ba604","metadata":{},"outputs":[],"source":["query_result[:5]"]},{"cell_type":"markdown","id":"976fcf83-8d11-4996-9d36-403abe38a183","metadata":{},"outputs":[],"source":["Do you see the differences between embeddings that are created by the watsonx embedding model and the Hugging Face embedding model?\n"]},{"cell_type":"markdown","id":"e7dc32ca-4500-455f-93e6-c1b194b9e298","metadata":{},"outputs":[],"source":["### Document embeddings\n"]},{"cell_type":"markdown","id":"820fbc22-0a8f-4a2e-940f-c24bb98cbc0e","metadata":{},"outputs":[],"source":["Next, you can do the same for creating embeddings from documents.\n"]},{"cell_type":"code","id":"d7a018f2-bd03-44a9-8d86-a064f301f2ba","metadata":{},"outputs":[],"source":["doc_result = huggingface_embedding.embed_documents(chunks)\ndoc_result[0][:5]"]},{"cell_type":"code","id":"c5f3407a-5985-4ebf-aa87-7173f454c6e5","metadata":{},"outputs":[],"source":["len(doc_result[0])"]},{"cell_type":"markdown","id":"d3b9f546-6084-4bc4-aee0-375c85de861a","metadata":{},"outputs":[],"source":["# Exercises\n"]},{"cell_type":"markdown","id":"c52235a8-aefd-4d15-80f6-2d56ea0341ab","metadata":{},"outputs":[],"source":["### Exercise 1 - Using another watsonx embedding model\n","Watsonx.ai also supports other embedding models, for which you can find more information [here](https://dataplatform.cloud.ibm.com/docs/content/wsj/analyze-data/fm-slate-30m-english-rtrvr-model-card.html?utm_source=skills_network&utm_content=in_lab_content_link&utm_id=Lab-Embed+documents+with+watsonx%E2%80%99s+embedding_v1_1721662184&context=wx). Can you try to use another embedding model to create embeddings for the document?\n"]},{"cell_type":"code","id":"f59dd9cf-8b52-4e1b-9ffc-28409913512a","metadata":{},"outputs":[],"source":["# Your code here"]},{"cell_type":"markdown","id":"d64f6bdc-33d8-4c04-ba38-57338dbebd2d","metadata":{},"outputs":[],"source":["<details>\n","    <summary>Click here for Solution</summary>\n","\n","```python\n","\n","from ibm_watsonx_ai.metanames import EmbedTextParamsMetaNames\n","from langchain_ibm import WatsonxEmbeddings\n","\n","embed_params = {\n","    EmbedTextParamsMetaNames.TRUNCATE_INPUT_TOKENS: 3,\n","    EmbedTextParamsMetaNames.RETURN_OPTIONS: {\"input_text\": True},\n","}\n","\n","watsonx_embedding = WatsonxEmbeddings(\n","    model_id=\"ibm/slate-30m-english-rtrvr\",\n","    url=\"https://us-south.ml.cloud.ibm.com\",\n","    project_id=\"skills-network\",\n","    params=embed_params,\n",")\n","\n","doc_result = watsonx_embedding.embed_documents(chunks)\n","\n","doc_result[0][:5]\n","\n","```\n","\n","</details>\n"]},{"cell_type":"markdown","id":"e84fb1c7-99a9-4a95-a1a2-9ff7b1d0b8ea","metadata":{},"outputs":[],"source":["## Authors\n"]},{"cell_type":"markdown","id":"1aecb6da-999b-4a4c-9418-a801b1ed8200","metadata":{},"outputs":[],"source":["[Kang Wang](https://author.skills.network/instructors/kang_wang)\n","\n","Kang Wang is a Data Scientist in IBM. He is also a PhD Candidate in the University of Waterloo.\n"]},{"cell_type":"markdown","id":"d8a9d8ac-4a98-478d-aa0a-38d1f61622cc","metadata":{},"outputs":[],"source":["### Other Contributors\n"]},{"cell_type":"markdown","id":"8ca8a409-e587-46fe-8820-034f4c784dcc","metadata":{},"outputs":[],"source":["[Joseph Santarcangelo](https://author.skills.network/instructors/joseph_santarcangelo)\n","\n","Joseph has a Ph.D. in Electrical Engineering, his research focused on using machine learning, signal processing, and computer vision to determine how videos impact human cognition. Joseph has been working for IBM since he completed his PhD.\n","\n","[Hailey Quach](https://author.skills.network/instructors/hailey_quach)\n","\n","Hailey is a Data Scientist Intern at IBM. She is also pursuing a BSc in Computer Science, Honors at Concordia University, Montreal.\n"]},{"cell_type":"markdown","id":"e4bd4ec5-f4a2-4912-87a1-df17aaa65d8f","metadata":{},"outputs":[],"source":["```{## Change Log}\n","```\n"]},{"cell_type":"markdown","id":"2e0cb0e3-c8d3-4905-b91a-9ccd3f7af19e","metadata":{},"outputs":[],"source":["```{|Date (YYYY-MM-DD)|Version|Changed By|Change Description||-|-|-|-||2024-07-22|0.1|Kang Wang|Create the lab|}\n","```\n"]},{"cell_type":"markdown","id":"dd27b7f8-6564-4451-b79f-6f5d20fb7746","metadata":{},"outputs":[],"source":["Copyright Â© IBM Corporation. All rights reserved.\n","\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.9"},"prev_pub_hash":"65f750ab814216e3147edda42117ed7e41055483dd85447ccc401a5a9bd9a612"},"nbformat":4,"nbformat_minor":4}